from csv import DictReader
from collections import defaultdict
import re, math

import os
import argparse
parser = argparse.ArgumentParser()

# parameter for gold standard labels (top N)
N = 10
output_gs_path = "../model_run/output_goldstandard"

# model parameters
parser.add_argument("-us", "--unsupervised", help ="evaluate unsupervised model", action = "store_true")
parser.add_argument("-usft", "--unsupervised_ft", help ="evaluate unsupervised fasttext model", action = "store_true")
parser.add_argument("-s", "--supervised", help ="evaluate supervised model", action = "store_true")

gold_topic_labels = defaultdict(lambda: {})
with open("annotated_dataset.csv", "r") as f:
    reader = DictReader(f, delimiter="\t")
    for row in reader:
        annotator_scores = [float(score) for field, score in row.items() \
            if "annotator" in field and score != ""]
        avg_score = sum(annotator_scores)/len(annotator_scores)
        topic_id = int(float(row["topic_id"]))
        gold_topic_labels[topic_id][row["label"].replace(" ", "_")] = avg_score

def generate_gs(n):
    g = open(output_gs_path , 'w')
    for topic_id, lbl_scores in gold_topic_labels.items():
        top_p = sorted(lbl_scores, key=lambda x: x[1], reverse=True)[:n]
        g.write("Top " +str(n)+ " labels for topic " +str(topic_id) +" are:" +"\n")
        for lbl in top_p:
            g.write(lbl +"\n")
        g.write("\n")
    g.close()
    
# generate gold standard labels for top N labels
generate_gs(N)

def parse_output_file(fname):
    """Parses labels for each topic from the output files generated by the Learn2Rank SVM. """
    d = defaultdict(lambda: [])
    with open(fname, "r") as f:
        curr_topic = None
        expected_count = 0
        curr_count = 0
        for line in f:
            line = line.strip()
            if curr_count < expected_count and curr_topic is not None:
                d[curr_topic].append(line)
                curr_count += 1
            elif curr_topic is None:
                m = re.match(r"Top ([0-9]+) labels for topic ([0-9]+) are:", line)
                if m:
                    expected_count = int(m.group(1))
                    curr_topic = int(m.group(2))
                    curr_count = 0
            else:
                curr_count = 0
                expected_count = 0
                curr_topic = None
    return d

def DCG_p(results, topic, p):
    """Computes DCG@p for given results and topic"""
    rel = lambda label: gold_topic_labels[topic][label]
    top_p = results[:p]
    dcg = 0
    for idx, label in enumerate(top_p):
        rank = idx + 1
        dcg += (2 ** (rel(label)) - 1) / math.log(rank + 1, 2)
    return dcg

def IDCG_p(topic, p):
    """ Computes the Idealized-DCG@p for a given topic. Based on gold standard rankings """
    lbl_scores = gold_topic_labels[topic].items() # (label, score) list
    top_p = sorted(lbl_scores, key=lambda x: x[1], reverse=True)[:p]
    idcg = 0
    for idx, (label, rel) in enumerate(top_p):
        rank = idx + 1
        idcg += (2 ** rel - 1) / math.log(rank + 1, 2)
    return idcg

def nDCG_p(results, topic, p):
    """
    Computes the normalized DCG@p (DCG scaled down by the truths)
    Source: https://en.wikipedia.org/wiki/Discounted_cumulative_gain
    nDCG_p = DCG_p / IDCG_p
    """
    return DCG_p(results, topic, p) / IDCG_p(topic, p)

def avg_nDCG_p(model_topic_labels, p):
    """ Compute the average nDCG@p from all the topics in model_topic_labels."""
    model_dcg_sum = 0
    N_docs = len(model_topic_labels)
    for topic_id, labels in model_topic_labels.items():
        model_dcg_sum += nDCG_p(labels, topic_id, p)
    return model_dcg_sum / N_docs

def top_1_avg(model_topic_labels):
    """ Compute the average top_1 score from all the topics in model_topic_labels."""
    model_sum = 0
    gold_sum = 0
    for topic_id, labels in model_topic_labels.items():
        top_model_label = labels[0]
        model_score = gold_topic_labels[topic_id][top_model_label]
        gold_score = max(gold_topic_labels[topic_id].values())
        model_sum += model_score
        gold_sum += gold_score
    N_topics = len(model_topic_labels)
    top1avg = model_sum/N_topics
    upper_bound = gold_sum/N_topics
    return (top1avg, upper_bound)

def evaluate_model(model_topic_labels):

    nDCG_1 = avg_nDCG_p(model_topic_labels, 1)
    nDCG_3 = avg_nDCG_p(model_topic_labels, 3)
    nDCG_5 = avg_nDCG_p(model_topic_labels, 5)
    (top_1_avg_score, upper_bound) = top_1_avg(model_topic_labels)

    print ("nDCG_1 : %.2f" % nDCG_1)
    print ("nDCG_3 : %.2f" % nDCG_3)
    print ("nDCG_5 : %.2f" % nDCG_5)
    print ("Top-1 Avg : %.2f" % top_1_avg_score)
    print ("Upper bound : %.2f \n" % upper_bound)

args = parser.parse_args()
if args.unsupervised:  
    print "\nEvaluting Unsupervised Model"
    fname = "../model_run/output_unsupervised"
    d = parse_output_file(fname)
    evaluate_model(d)

elif args.unsupervised_ft:
    print "\nEvaluating Unsupervised Model using fasttext"
    fname = "../model_run/output_unsupervised_ft"
    d = parse_output_file(fname)
    evaluate_model(d)

elif args.supervised:  
    print "\nEvluating Supervised Model"
    fname = "../model_run/output_supervised"
    d = parse_output_file(fname)
    evaluate_model(d)

else:
    print "Invalid model evaluation"